{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import os\n",
    "import matplotlib.pyplot as plt \n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'C:\\Users\\jgaur\\Tensorflow_Tut\\NLP\\train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3271\n",
      "4342\n"
     ]
    }
   ],
   "source": [
    "print((df.target == 1).sum()) # disaster\n",
    "print((df.target == 0).sum()) # no disaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing\n",
    "\n",
    "import re # Regular Expression\n",
    "import string\n",
    "\n",
    "    def remove_url(text):\n",
    "        url = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
    "        return url.sub(r\"\", text)\n",
    "\n",
    "def remove_punc(text):\n",
    "    translator = str.maketrans(\"\", \"\", string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "string.punctuation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "@bbcmtd Wholesale Markets ablaze http://t.co/lHYXEOHY6C\n",
      "2\n",
      "t\n",
      "3\n",
      "@bbcmtd Wholesale Markets ablaze \n"
     ]
    }
   ],
   "source": [
    "pattern = re.compile(r\"https?://(\\S+|www)\\.\\S+\")\n",
    "for t in df.text:\n",
    "    matches = pattern.findall(t)\n",
    "    for match in matches:\n",
    "        print(\"1\")\n",
    "        print(t)\n",
    "        print(\"2\")\n",
    "        print(match)\n",
    "        print(\"3\")\n",
    "        print(pattern.sub(r\"\", t))\n",
    "    if len(matches) > 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this earthquake Ma...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to shelter in place are be...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13000 people receive wildfires evacuation orde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby Alaska as s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this earthquake Ma...   \n",
       "1   4     NaN      NaN              Forest fire near La Ronge Sask Canada   \n",
       "2   5     NaN      NaN  All residents asked to shelter in place are be...   \n",
       "3   6     NaN      NaN  13000 people receive wildfires evacuation orde...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby Alaska as s...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'] = df.text.map(remove_url)        # map(lambda x: remove_url(x))\n",
    "df['text'] = df.text.map(remove_punc)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jgaur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# remove stopwords \n",
    "# (Stopwords are the English words which does not add much meaning to a sentence. They can safely be ignored           without sacrificing the meaning of the sentence. For example, the words like the, he, have etc. Such words are      already captured this in corpus named corpus.\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = set(stopwords.words(\"english\"))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    filtered_words = [word.lower() for word in text.split() if word.lower() not in stop]\n",
    "    return \" \".join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            deeds reason earthquake may allah forgive us\n",
       "1                   forest fire near la ronge sask canada\n",
       "2       residents asked shelter place notified officer...\n",
       "3       13000 people receive wildfires evacuation orde...\n",
       "4       got sent photo ruby alaska smoke wildfires pou...\n",
       "                              ...                        \n",
       "7608    two giant cranes holding bridge collapse nearb...\n",
       "7609    ariaahrary thetawniest control wild fires cali...\n",
       "7610                      m194 0104 utc5km volcano hawaii\n",
       "7611    police investigating ebike collided car little...\n",
       "7612    latest homes razed northern california wildfir...\n",
       "Name: text, Length: 7613, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"text\"] = df.text.map(remove_stopwords)\n",
    "df.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Count unique words\n",
    "def counter_word(text_col):\n",
    "    count = Counter()\n",
    "    for text in text_col.values:\n",
    "        for word in text.split():\n",
    "            count[word] += 1\n",
    "    return count\n",
    "\n",
    "counter = counter_word(df.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17971"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('like', 345), ('im', 299), ('amp', 298), ('fire', 250), ('get', 229)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_unique_words = len(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(df.shape[0] * 0.8)\n",
    "train_df = df[:train_size]\n",
    "val_df = df[train_size:]\n",
    "\n",
    "train_sentence = train_df.text.to_numpy()\n",
    "train_labels = train_df.target.to_numpy()\n",
    "val_sentence = val_df.text.to_numpy()\n",
    "val_labels = val_df.target.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['deeds reason earthquake may allah forgive us',\n",
       "       'forest fire near la ronge sask canada',\n",
       "       'residents asked shelter place notified officers evacuation shelter place orders expected',\n",
       "       ..., 'feel like sinking unhappiness take quiz',\n",
       "       'sinking music video tv career brooke hogan thanking dad free publicityalthough doubt help',\n",
       "       'supernovalester feel bad literally feel feeling heart sinking bc didnt get anyone ugh jfc'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentence.shape, val_sentence.shape\n",
    "train_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize \n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# vectorize a text corpus by turning each text into a sequence of interger\n",
    "tokenizer = Tokenizer(num_words=num_unique_words)\n",
    "tokenizer.fit_on_texts(train_sentence)  # fit only to training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each word has unique index\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequence = tokenizer.texts_to_sequences(train_sentence)\n",
    "val_sequence = tokenizer.texts_to_sequences(val_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['deeds reason earthquake may allah forgive us'\n",
      " 'forest fire near la ronge sask canada'\n",
      " 'residents asked shelter place notified officers evacuation shelter place orders expected'\n",
      " '13000 people receive wildfires evacuation orders california'\n",
      " 'got sent photo ruby alaska smoke wildfires pours school']\n",
      "[[3739, 696, 235, 41, 1282, 3740, 14], [71, 3, 129, 576, 5670, 5671, 1283], [1448, 1186, 1882, 495, 5672, 1449, 116, 1882, 495, 976, 1187], [2243, 8, 3741, 1070, 116, 976, 24], [27, 1071, 358, 5673, 1635, 892, 1070, 5674, 91]]\n"
     ]
    }
   ],
   "source": [
    "print(train_sentence[0:5])\n",
    "print(train_sequence[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6090, 20), (1523, 20))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pad the sentence to have the same length\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Maximum number of words in a suquence\n",
    "max_length = 20\n",
    "\n",
    "train_padded = pad_sequences(train_sequence, maxlen=max_length, padding=\"post\", truncating=\"post\")\n",
    "val_padded = pad_sequences(val_sequence, maxlen=max_length, padding=\"post\", truncating=\"post\")\n",
    "train_padded.shape, val_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3739,  696,  235,   41, 1282, 3740,   14,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_padded[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "three people died heat wave far\n",
      "[520, 8, 395, 156, 297, 411]\n",
      "[520   8 395 156 297 411   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0]\n"
     ]
    }
   ],
   "source": [
    "print(train_sentence[10])\n",
    "print(train_sequence[10])\n",
    "print(train_padded[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check reversing the indices\n",
    "\n",
    "# flip key, values\n",
    "reverse_word_index = dict([(idx, word) for word, idx in word_index.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reverse_word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(sequence):\n",
    "    return ' '.join([reverse_word_index.get(idx, \"?\") for idx in sequence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[520, 8, 395, 156, 297, 411]\n",
      "three people died heat wave far\n"
     ]
    }
   ],
   "source": [
    "decode_text = decode(train_sequence[10])\n",
    "print(train_sequence[10])\n",
    "print(decode_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 20, 32)            575072    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 64)                24832     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 599,969\n",
      "Trainable params: 599,969\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create LSTM Model\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Embedding: https://www.tensorflow.org/tutorials/text/word_embeddings\n",
    "# Turns positive integers (indexes) into dense vectors of fixed size. (other approach could be one-hot-encoding)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(layers.Embedding(num_unique_words, 32, input_length=max_length))\n",
    "\n",
    "# The layer will take as input an integer matrix of size (batch, input_length)\n",
    "# and  the largest integer (i.e. word index) in the input should be no longer than num_words (vocabulary size)\n",
    "# Now model.output_shape is (None, input_length, 32), where None is tha batch dimension\n",
    "\n",
    "model.add(layers.LSTM(64, dropout=0.1))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "optim = keras.optimizers.Adam(lr=0.001)\n",
    "metrics = ['accuracy']\n",
    "\n",
    "model.compile(loss=loss, optimizer=optim, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "191/191 - 6s - loss: 0.5350 - accuracy: 0.7241 - val_loss: 0.4956 - val_accuracy: 0.7630\n",
      "Epoch 2/20\n",
      "191/191 - 6s - loss: 0.2864 - accuracy: 0.8903 - val_loss: 0.5628 - val_accuracy: 0.7663\n",
      "Epoch 3/20\n",
      "191/191 - 5s - loss: 0.1507 - accuracy: 0.9489 - val_loss: 0.6378 - val_accuracy: 0.7498\n",
      "Epoch 4/20\n",
      "191/191 - 5s - loss: 0.1039 - accuracy: 0.9673 - val_loss: 0.6368 - val_accuracy: 0.7479\n",
      "Epoch 5/20\n",
      "191/191 - 6s - loss: 0.0864 - accuracy: 0.9732 - val_loss: 0.7689 - val_accuracy: 0.7334\n",
      "Epoch 6/20\n",
      "191/191 - 6s - loss: 0.0732 - accuracy: 0.9772 - val_loss: 0.8256 - val_accuracy: 0.7321\n",
      "Epoch 7/20\n",
      "191/191 - 7s - loss: 0.0649 - accuracy: 0.9777 - val_loss: 0.9546 - val_accuracy: 0.7255\n",
      "Epoch 8/20\n",
      "191/191 - 7s - loss: 0.0534 - accuracy: 0.9782 - val_loss: 1.1011 - val_accuracy: 0.7393\n",
      "Epoch 9/20\n",
      "191/191 - 9s - loss: 0.0477 - accuracy: 0.9796 - val_loss: 0.8947 - val_accuracy: 0.7288\n",
      "Epoch 10/20\n",
      "191/191 - 8s - loss: 0.0410 - accuracy: 0.9819 - val_loss: 1.1788 - val_accuracy: 0.7387\n",
      "Epoch 11/20\n",
      "191/191 - 7s - loss: 0.0363 - accuracy: 0.9831 - val_loss: 1.3008 - val_accuracy: 0.7446\n",
      "Epoch 12/20\n",
      "191/191 - 7s - loss: 0.0387 - accuracy: 0.9828 - val_loss: 1.2571 - val_accuracy: 0.7203\n",
      "Epoch 13/20\n",
      "191/191 - 5s - loss: 0.0357 - accuracy: 0.9823 - val_loss: 1.5327 - val_accuracy: 0.7249\n",
      "Epoch 14/20\n",
      "191/191 - 6s - loss: 0.0338 - accuracy: 0.9841 - val_loss: 1.8678 - val_accuracy: 0.7216\n",
      "Epoch 15/20\n",
      "191/191 - 6s - loss: 0.0422 - accuracy: 0.9816 - val_loss: 1.3540 - val_accuracy: 0.7295\n",
      "Epoch 16/20\n",
      "191/191 - 6s - loss: 0.0448 - accuracy: 0.9818 - val_loss: 1.1771 - val_accuracy: 0.7150\n",
      "Epoch 17/20\n",
      "191/191 - 6s - loss: 0.0364 - accuracy: 0.9823 - val_loss: 1.1933 - val_accuracy: 0.7400\n",
      "Epoch 18/20\n",
      "191/191 - 6s - loss: 0.0320 - accuracy: 0.9837 - val_loss: 1.5888 - val_accuracy: 0.7249\n",
      "Epoch 19/20\n",
      "191/191 - 6s - loss: 0.0354 - accuracy: 0.9828 - val_loss: 1.8026 - val_accuracy: 0.7216\n",
      "Epoch 20/20\n",
      "191/191 - 6s - loss: 0.0416 - accuracy: 0.9813 - val_loss: 1.3057 - val_accuracy: 0.7229\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1e12a269d60>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_padded, train_labels, epochs=20, validation_data=(val_padded, val_labels), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(train_padded)\n",
    "predictions = [1 if p> 0.5 else 0 for p in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['three people died heat wave far'\n",
      " 'haha south tampa getting flooded hah wait second live south tampa gonna gonna fvck flooding'\n",
      " 'raining flooding florida tampabay tampa 18 19 days ive lost count'\n",
      " 'flood bago myanmar arrived bago'\n",
      " 'damage school bus 80 multi car crash breaking' 'whats man' 'love fruits'\n",
      " 'summer lovely' 'car fast' 'goooooooaaaaaal']\n"
     ]
    }
   ],
   "source": [
    "print(train_sentence[10:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 0 0 0 0 0]\n",
      "[1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(train_labels[10:20])\n",
    "print(predictions[10:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
